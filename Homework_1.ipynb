{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Сборщик научно-популярного русскоязычного корпуса текстов (2025–2026)\n",
        "# Темы: ИИ, нейросети, космос, биотехнологии, квантовая физика, климат, энергетика\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ========================================\n",
        "# Установка зависимостей (если нужно)\n",
        "# ========================================\n",
        "# %pip install -q requests fake-useragent beautifulsoup4 pymorphy3 nltk pandas tqdm\n",
        "\n",
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy3\n",
        "from urllib.parse import quote_plus, urlparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Инициализация...\")\n",
        "\n",
        "# NLTK ресурсы\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Морфология\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "ru_stop = set(stopwords.words('russian'))\n",
        "\n",
        "print(\"Библиотеки готовы\\n\")\n",
        "\n",
        "# ========================================\n",
        "# СПИСОК ТЕМ И КЛЮЧЕВЫХ СЛОВ (2025–2026)\n",
        "# ========================================\n",
        "THEMES = [\n",
        "    \"искусственный интеллект 2025\", \"нейросети 2026\", \"генеративный ИИ\",\n",
        "    \"квантовые компьютеры\", \"квантовая криптография\", \"квантовые вычисления\",\n",
        "    \"термоядерный синтез\", \"ядерный синтез 2026\", \"энергия будущего\",\n",
        "    \"изменение климата 2025\", \"углеродная нейтральность\", \"зелёная энергетика\",\n",
        "    \"биотехнологии\", \"генетическое редактирование\", \"CRISPR 2026\",\n",
        "    \"космические миссии 2025\", \"колонизация Марса\", \"Starship\", \"лунная база\",\n",
        "    \"нейроморфные чипы\", \"brain-computer interface\", \"нейроинтерфейсы\",\n",
        "    \"метавселенная 2026\", \"Web3\", \"децентрализованные финансы\",\n",
        "    \"робототехника человекоподобные\", \"гуманоидные роботы\",\n",
        "    \"автономные автомобили 2026\", \"беспилотники уровень 5\",\n",
        "    \"долгожительство\", \"биохакинг\", \"омоложение\",\n",
        "    \"космический туризм\", \"орбитальные отели\", \"SpaceX\",\n",
        "    \"водородная энергетика\", \"батареи нового поколения\", \"твердотельные аккумуляторы\"\n",
        "]\n",
        "\n",
        "# ========================================\n",
        "# ПОИСК ЧЕРЕЗ ЯНДЕКС (основной источник)\n",
        "# ========================================\n",
        "def search_yandex(query, count=20):\n",
        "    try:\n",
        "        url = f\"https://yandex.ru/search/?text={quote_plus(query)}&lr=213\"\n",
        "        headers = {'User-Agent': UserAgent().random}\n",
        "        r = requests.get(url, headers=headers, timeout=12)\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "        links = []\n",
        "        for item in soup.select('li.serp-item a.Link'):\n",
        "            href = item.get('href')\n",
        "            if href and href.startswith('http') and 'yandex' not in href:\n",
        "                links.append(href)\n",
        "\n",
        "        print(f\"Яндекс → {len(links)} ссылок по '{query}'\")\n",
        "        return links[:count]\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка Яндекса: {e}\")\n",
        "        return []\n",
        "\n",
        "# ========================================\n",
        "# БАЗОВЫЕ НАУЧНЫЕ САЙТЫ (прямые ссылки)\n",
        "# ========================================\n",
        "def get_seed_urls():\n",
        "    seeds = []\n",
        "    sites = [\n",
        "        \"https://nplus1.ru/search?query={q}\",\n",
        "        \"https://habr.com/ru/search/?q={q}\",\n",
        "        \"https://naked-science.ru/search?query={q}\",\n",
        "        \"https://elementy.ru/search?query={q}\",\n",
        "        \"https://postnauka.ru/search?query={q}\",\n",
        "        \"https://scientificrussia.ru/search?query={q}\",\n",
        "        \"https://trends.rbc.ru/search?query={q}\",\n",
        "        \"https://vc.ru/search/v2/content?query={q}\",\n",
        "    ]\n",
        "\n",
        "    for q in THEMES[:30]:\n",
        "        for site in sites:\n",
        "            try:\n",
        "                url = site.format(q=quote_plus(q))\n",
        "                seeds.append(url)\n",
        "            except:\n",
        "                pass\n",
        "    return seeds\n",
        "\n",
        "# ========================================\n",
        "# СОБИРАЕМ URL\n",
        "# ========================================\n",
        "def collect_links(target=1200):\n",
        "    print(f\"Собираем ссылки... Цель: {target}\")\n",
        "    urls = set()\n",
        "\n",
        "    # 1. Прямые запросы по темам\n",
        "    for theme in tqdm(THEMES, desc=\"По темам\"):\n",
        "        links = search_yandex(theme, 25)\n",
        "        urls.update(links)\n",
        "        time.sleep(random.uniform(1.8, 4.2))\n",
        "\n",
        "    # 2. Семенные сайты\n",
        "    print(\"\\nДобавляем прямые ссылки с научных сайтов...\")\n",
        "    seeds = get_seed_urls()\n",
        "    urls.update(seeds)\n",
        "\n",
        "    # Фильтрация мусора\n",
        "    bad = {'youtube.com', 'vk.com', 't.me', 'facebook.com', 'instagram.com', 'pikabu.ru'}\n",
        "    filtered = [u for u in urls if not any(x in u for x in bad)]\n",
        "\n",
        "    print(f\"Всего собрано уникальных: {len(filtered)}\")\n",
        "    return filtered[:target]\n",
        "\n",
        "# ========================================\n",
        "# ПАРСИНГ СТРАНИЦЫ\n",
        "# ========================================\n",
        "def scrape_page(url, session):\n",
        "    try:\n",
        "        r = session.get(url, timeout=14)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "        # Удаляем мусор\n",
        "        for el in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'iframe']):\n",
        "            el.decompose()\n",
        "\n",
        "        title = (soup.title.string or \"Без заголовка\").strip()\n",
        "\n",
        "        # Текст — самые вероятные контейнеры\n",
        "        candidates = soup.find_all(['article', 'div', 'section'], class_=re.compile(r'(article|post|content|entry|text|body|main)'))\n",
        "        if not candidates:\n",
        "            candidates = soup.find_all('p')\n",
        "\n",
        "        paragraphs = [p.get_text(strip=True) for p in candidates if len(p.get_text(strip=True)) > 40]\n",
        "        text = ' '.join(paragraphs)\n",
        "\n",
        "        if len(text) < 300:\n",
        "            return None\n",
        "\n",
        "        # Дата (попытка)\n",
        "        date_tag = soup.find('meta', {'property': 'article:published_time'}) or \\\n",
        "                   soup.find('meta', {'name': 'date'}) or \\\n",
        "                   soup.find('time', {'datetime': True})\n",
        "\n",
        "        date = datetime.now().strftime('%Y-%m-%d')\n",
        "        if date_tag:\n",
        "            dt = date_tag.get('content') or date_tag.get('datetime')\n",
        "            if dt:\n",
        "                try:\n",
        "                    date = datetime.fromisoformat(dt.split('T')[0]).strftime('%Y-%m-%d')\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return {'url': url, 'title': title, 'text': text[:12000], 'date': date}\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# ========================================\n",
        "# ОЧИСТКА И ЛЕММАТИЗАЦИЯ\n",
        "# ========================================\n",
        "def process_text(raw):\n",
        "    text = re.sub(r'<[^>]+>', ' ', raw)\n",
        "    text = re.sub(r'\\d+[.,]?\\d*', ' ', text)\n",
        "    text = re.sub(r'[«»—–…“”„‟′‶`´]+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    lemmas = []\n",
        "    for t in tokens:\n",
        "        if t.isalpha() and len(t) > 2 and t not in ru_stop:\n",
        "            p = morph.parse(t)[0]\n",
        "            lemmas.append(p.normal_form)\n",
        "\n",
        "    return ' '.join(lemmas), len(lemmas)\n",
        "\n",
        "# ========================================\n",
        "# ОСНОВНОЙ ЦИКЛ\n",
        "# ========================================\n",
        "def run_scraper(max_docs=800):\n",
        "    print(f\"\\n=== СБОР НАУЧНО-ПОП КОРПУСА 2025–2026 ===\\nЦель: {max_docs} документов\\n\")\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers['User-Agent'] = UserAgent().random\n",
        "\n",
        "    links = collect_links(target=max_docs * 2)\n",
        "    random.shuffle(links)\n",
        "\n",
        "    documents = []\n",
        "    seen = set()\n",
        "\n",
        "    for url in tqdm(links, desc=\"Парсинг страниц\"):\n",
        "        if len(documents) >= max_docs:\n",
        "            break\n",
        "        if url in seen:\n",
        "            continue\n",
        "\n",
        "        data = scrape_page(url, session)\n",
        "        if not data:\n",
        "            continue\n",
        "\n",
        "        cleaned, count = process_text(data['text'])\n",
        "        if count < 40:\n",
        "            continue\n",
        "\n",
        "        documents.append({\n",
        "            'id': len(documents) + 1,\n",
        "            'title': data['title'][:220],\n",
        "            'url': url,\n",
        "            'date': data['date'],\n",
        "            'raw_length': len(data['text']),\n",
        "            'tokens': count,\n",
        "            'cleaned': cleaned,\n",
        "            'source': urlparse(url).netloc\n",
        "        })\n",
        "\n",
        "        seen.add(url)\n",
        "        time.sleep(random.uniform(1.1, 3.3))\n",
        "\n",
        "    print(f\"\\nСобрано документов: {len(documents)}\")\n",
        "\n",
        "    # Сохранение\n",
        "    df = pd.DataFrame(documents)\n",
        "    df.to_csv('science_corpus_2026.csv', index=False, encoding='utf-8-sig')\n",
        "    print(\"→ CSV сохранён\")\n",
        "\n",
        "    with open('science_corpus_2026.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(documents, f, ensure_ascii=False, indent=2)\n",
        "    print(\"→ JSON сохранён\")\n",
        "\n",
        "    # TXT-версия для чтения\n",
        "    with open('science_corpus_2026.txt', 'w', encoding='utf-8') as f:\n",
        "        for doc in documents:\n",
        "            f.write(f\"{'='*90}\\n\")\n",
        "            f.write(f\"ID {doc['id']} | {doc['date']} | {doc['source']}\\n\")\n",
        "            f.write(f\"{doc['title']}\\n\")\n",
        "            f.write(f\"URL: {doc['url']}\\n\")\n",
        "            f.write(f\"Токенов: {doc['tokens']}\\n\")\n",
        "            f.write(f\"{'-'*90}\\n\")\n",
        "            f.write(f\"{doc['cleaned'][:1500]}...\\n\\n\")\n",
        "\n",
        "    print(\"→ TXT сохранён\")\n",
        "\n",
        "    # Статистика\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"СТАТИСТИКА КОРПУСА\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Документов:          {len(documents)}\")\n",
        "    print(f\"Общее токенов:       {df['tokens'].sum():,}\")\n",
        "    print(f\"Среднее токенов/док: {df['tokens'].mean():.1f}\")\n",
        "    print(f\"Уникальных источников: {df['source'].nunique()}\")\n",
        "    print(\"\\nТоп-10 источников:\")\n",
        "    print(df['source'].value_counts().head(10))\n",
        "\n",
        "    print(\"\\nГотово! Корпус можно использовать для тематического моделирования, классификации и т.д.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_scraper(max_docs=750)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
